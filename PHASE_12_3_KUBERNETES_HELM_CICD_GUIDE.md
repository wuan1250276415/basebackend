# Phase 12.3: å®¹å™¨åŒ–ä¸ç¼–æ’å®æ–½æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—ä»‹ç»å¦‚ä½•å®æ–½ Kubernetes å®¹å™¨åŒ–ç¼–æ’ï¼ŒåŒ…æ‹¬é›†ç¾¤æ­å»ºã€Helm Chart æ‰“åŒ…ã€CI/CD æµæ°´çº¿ç­‰æ ¸å¿ƒèƒ½åŠ›ï¼Œæ„å»ºç°ä»£åŒ–çš„å®¹å™¨åŒ–éƒ¨ç½²å¹³å°ã€‚

---

## ğŸ—ï¸ Kubernetes é›†ç¾¤æ¶æ„

### æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Kubernetes é›†ç¾¤æ¶æ„                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚  Master èŠ‚ç‚¹  â”‚  â”‚ Worker èŠ‚ç‚¹  â”‚  â”‚ Worker èŠ‚ç‚¹  â”‚           â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚           â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚           â”‚
â”‚  â”‚  â”‚ API    â”‚  â”‚  â”‚  â”‚ Kubeletâ”‚  â”‚  â”‚  â”‚ Kubeletâ”‚  â”‚           â”‚
â”‚  â”‚  â”‚ Server â”‚  â”‚  â”‚  â”‚        â”‚  â”‚  â”‚  â”‚        â”‚  â”‚           â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚           â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚           â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚           â”‚
â”‚  â”‚  â”‚ Schedulerâ”‚  â”‚  â”‚  Containerdâ”‚  â”‚  â”‚ Containerdâ”‚  â”‚           â”‚
â”‚  â”‚  â”‚         â”‚  â”‚  â”‚  â”‚         â”‚  â”‚  â”‚  â”‚         â”‚  â”‚           â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚           â”‚
â”‚  â”‚              â”‚  â”‚              â”‚  â”‚              â”‚           â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚           â”‚
â”‚  â”‚  â”‚ etcd   â”‚  â”‚  â”‚  â”‚Pod    â”‚  â”‚  â”‚  â”‚Pod    â”‚  â”‚           â”‚
â”‚  â”‚  â”‚        â”‚  â”‚  â”‚  â”‚       â”‚  â”‚  â”‚  â”‚       â”‚  â”‚           â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚         â”‚                 â”‚                 â”‚                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚  â”‚              å­˜å‚¨å±‚                              â”‚             â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚             â”‚
â”‚  â”‚  â”‚ NFS    â”‚ â”‚ Ceph   â”‚ â”‚ åˆ†å¸ƒå¼  â”‚            â”‚             â”‚
â”‚  â”‚  â”‚        â”‚ â”‚        â”‚ â”‚ å­˜å‚¨     â”‚            â”‚             â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚             â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜             â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                  é™„åŠ ç»„ä»¶å±‚                                   â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ â€¢ Ingress Controller (Nginx/Traefik)                         â”‚ â”‚
â”‚  â”‚ â€¢ Service Mesh (Istio)                                      â”‚ â”‚
â”‚  â”‚ â€¢ Prometheus + Grafana (ç›‘æ§)                               â”‚ â”‚
â”‚  â”‚ â€¢ ELK Stack (æ—¥å¿—)                                           â”‚ â”‚
â”‚  â”‚ â€¢ Harbor (é•œåƒä»“åº“)                                           â”‚ â”‚
â”‚  â”‚ â€¢ Registry (ç§æœ‰ä»“åº“)                                         â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### é›†ç¾¤ç»„ä»¶

| ç»„ä»¶ | ä½œç”¨ | éƒ¨ç½²èŠ‚ç‚¹ |
|------|------|----------|
| **kube-apiserver** | Kubernetes API æœåŠ¡ | Master |
| **kube-scheduler** | Pod è°ƒåº¦å™¨ | Master |
| **kube-controller-manager** | æ§åˆ¶å™¨ç®¡ç† | Master |
| **etcd** | åˆ†å¸ƒå¼é”®å€¼å­˜å‚¨ | Master |
| **kubelet** | èŠ‚ç‚¹ä»£ç† | Worker |
| **kube-proxy** | ç½‘ç»œä»£ç† | Worker |
| **Container Runtime** | å®¹å™¨è¿è¡Œæ—¶ | Worker |

---

## ğŸš€ Kubernetes é›†ç¾¤æ­å»º

### 1. ç¯å¢ƒå‡†å¤‡è„šæœ¬

```bash
#!/bin/bash
# ===================================================================
# Kubernetes é›†ç¾¤æ­å»ºè„šæœ¬
# ===================================================================

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() {
    echo -e "${BLUE}[INFO]${NC} $1"
}

log_success() {
    echo -e "${GREEN}[SUCCESS]${NC} $1"
}

log_warn() {
    echo -e "${YELLOW}[WARN]${NC} $1"
}

log_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# å®‰è£…ä¾èµ–
install_dependencies() {
    log_info "å®‰è£…ç³»ç»Ÿä¾èµ–..."

    apt-get update
    apt-get install -y \
        apt-transport-https \
        ca-certificates \
        curl \
        gnupg \
        lsb-release \
        conntrack \
        ebtables \
        ethtool \
        socat \
        util-linux

    log_success "ä¾èµ–å®‰è£…å®Œæˆ"
}

# å®‰è£… Docker
install_docker() {
    log_info "å®‰è£… Docker..."

    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg

    echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | tee /etc/apt/sources.list.d/docker.list > /dev/null

    apt-get update
    apt-get install -y docker-ce docker-ce-cli containerd.io

    # é…ç½® Docker
    cat > /etc/docker/daemon.json <<EOF
{
  "log-driver": "json-file",
  "log-opts": {
    "max-size": "100m",
    "max-file": "5"
  },
  "storage-driver": "overlay2",
  "live-restore": true,
  "userland-proxy": false,
  "experimental": false
}
EOF

    systemctl restart docker
    systemctl enable docker

    log_success "Docker å®‰è£…å®Œæˆ"
}

# å®‰è£… containerd
install_containerd() {
    log_info "å®‰è£… containerd..."

    apt-get update
    apt-get install -y containerd.io

    # é…ç½® containerd
    mkdir -p /etc/containerd
    containerd config default > /etc/containerd/config.toml

    sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

    systemctl restart containerd
    systemctl enable containerd

    log_success "containerd å®‰è£…å®Œæˆ"
}

# å®‰è£… kubectl
install_kubectl() {
    log_info "å®‰è£… kubectl..."

    curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"

    install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl

    log_success "kubectl å®‰è£…å®Œæˆ"
}

# å®‰è£… kubeadm
install_kubeadm() {
    log_info "å®‰è£… kubeadm å’Œ kubelet..."

    curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | \
        gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

    echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /" | \
        tee /etc/apt/sources.list.d/kubernetes.list

    apt-get update
    apt-get install -y kubelet kubeadm kubectl

    # é”å®šç‰ˆæœ¬
    apt-mark hold kubelet kubeadm kubectl

    log_success "kubeadm å®‰è£…å®Œæˆ"
}

# ç¦ç”¨ swap
disable_swap() {
    log_info "ç¦ç”¨ swap..."

    swapoff -a

    # æ°¸ä¹…ç¦ç”¨
    sed -i '/ swap / s/^\(.*\)$/#\1/g' /etc/fstab

    log_success "swap å·²ç¦ç”¨"
}

# é…ç½®å†…æ ¸å‚æ•°
configure_kernel() {
    log_info "é…ç½®å†…æ ¸å‚æ•°..."

    cat <<EOF > /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

    cat <<EOF > /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
net.netfilter.nf_conntrack_max     = 131072
EOF

    modprobe overlay
    modprobe br_netfilter
    sysctl --system

    log_success "å†…æ ¸å‚æ•°é…ç½®å®Œæˆ"
}

# åˆå§‹åŒ– Kubernetes é›†ç¾¤ï¼ˆä»…é™ Masterï¼‰
init_kubernetes_master() {
    log_info "åˆå§‹åŒ– Kubernetes é›†ç¾¤..."

    local POD_NETWORK="10.244.0.0/16"
    local SERVICE_NETWORK="10.96.0.0/12"

    kubeadm init \
        --pod-network-cidr=$POD_NETWORK \
        --service-cidr=$SERVICE_NETWORK \
        --apiserver-advertise-address=$(hostname -I | awk '{print $1}') \
        --kubernetes-version=1.29.0 \
        --cri-socket=unix:///var/run/containerd/containerd.sock

    # é…ç½® kubectl
    mkdir -p $HOME/.kube
    cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    chown $(id -u):$(id -g) $HOME/.kube/config

    log_success "Kubernetes é›†ç¾¤åˆå§‹åŒ–å®Œæˆ"
}

# éƒ¨ç½²ç½‘ç»œæ’ä»¶ï¼ˆFlannelï¼‰
deploy_network_plugin() {
    log_info "éƒ¨ç½² Flannel ç½‘ç»œæ’ä»¶..."

    kubectl apply -f https://github.com/flannel-io/flannel/releases/download/v0.25.1/kube-flannel.yml

    log_success "Flannel ç½‘ç»œæ’ä»¶éƒ¨ç½²å®Œæˆ"
}

# æ·»åŠ  Worker èŠ‚ç‚¹
join_worker_nodes() {
    log_info "æç¤ºæ·»åŠ  Worker èŠ‚ç‚¹..."
    log_info "åœ¨ Worker èŠ‚ç‚¹ä¸Šè¿è¡Œä»¥ä¸‹å‘½ä»¤åŠ å…¥é›†ç¾¤:"
    log_info "kubeadm join <MASTER_IP>:6443 --token <TOKEN> --discovery-token-ca-cert-hash sha256:<HASH>"

    # ç”ŸæˆåŠ å…¥å‘½ä»¤
    local token=$(kubeadm token list --format table | tail -n 1 | awk '{print $1}')
    local hash=$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | \
        openssl rsa -pubin -outform der 2>/dev/null | \
        openssl dgst -sha256 -hex | sed 's/^.* //')

    echo ""
    echo "============================================"
    echo "Worker èŠ‚ç‚¹åŠ å…¥å‘½ä»¤ï¼š"
    echo "============================================"
    echo "kubeadm join $(hostname -I | awk '{print $1}'):6443 --token $token --discovery-token-ca-cert-hash sha256:$hash"
    echo "============================================"
    echo ""
}

# éªŒè¯å®‰è£…
verify_installation() {
    log_info "éªŒè¯ Kubernetes å®‰è£…..."

    # æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
    kubectl get nodes

    # æ£€æŸ¥ç³»ç»Ÿç»„ä»¶
    kubectl get pods -n kube-system

    log_success "Kubernetes å®‰è£…éªŒè¯å®Œæˆ"
}

# ä¸»å‡½æ•°
main() {
    local NODE_TYPE=${1:-"master"}

    if [ "$NODE_TYPE" == "master" ]; then
        install_dependencies
        install_docker
        install_containerd
        install_kubectl
        install_kubeadm
        disable_swap
        configure_kernel
        init_kubernetes_master
        deploy_network_plugin
        verify_installation
        join_worker_nodes
    else
        install_dependencies
        install_docker
        install_containerd
        install_kubeadm
        disable_swap
        configure_kernel
        log_info "è¿è¡Œä»¥ä¸‹å‘½ä»¤åŠ å…¥é›†ç¾¤:"
        log_info "kubeadm join <MASTER_IP>:6443 --token <TOKEN> --discovery-token-ca-cert-hash sha256:<HASH>"
    fi

    log_success "é›†ç¾¤æ­å»ºå®Œæˆï¼"
}

main "$@"
```

### 2. Kubernetes é«˜å¯ç”¨é›†ç¾¤

```yaml
# ha-cluster.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: haproxy-config
  namespace: kube-system
data:
  haproxy.cfg: |
    global
      daemon
      maxconn 4096

    defaults
      mode http
      timeout connect 5000ms
      timeout client 50000ms
      timeout server 50000ms

    frontend kubernetes-frontend
      bind *:6443
      default_backend kubernetes-backend

    backend kubernetes-backend
      balance roundrobin
      server master1 192.168.1.10:6443 check
      server master2 192.168.1.11:6443 check
      server master3 192.168.1.12:6443 check
```

### 3. Ingress Controller éƒ¨ç½²

```yaml
# nginx-ingress.yaml
apiVersion: networking.k8s.io/v1
kind: IngressClass
metadata:
  name: nginx
spec:
  controller: k8s.io/ingress-nginx

---
apiVersion: v1
kind: Namespace
metadata:
  name: ingress-nginx

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: ingress-nginx
  namespace: ingress-nginx

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: ingress-nginx
rules:
  - apiGroups: [""]
    resources: ["configmaps", "endpoints", "nodes", "pods", "secrets"]
    verbs: ["list", "watch"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["get"]
  - apiGroups: [""]
    resources: ["services"]
    verbs: ["get", "list", "watch", "update"]
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses"]
    verbs: ["get", "list", "watch"]
  - apiGroups: [""]
    resources: ["events"]
    verbs: ["create", "patch"]
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses/status"]
    verbs: ["update"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: ingress-nginx
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: ingress-nginx
subjects:
  - kind: ServiceAccount
    name: ingress-nginx
    namespace: ingress-nginx

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app.kubernetes.io/name: ingress-nginx
      app.kubernetes.io/component: controller
  template:
    metadata:
      labels:
        app.kubernetes.io/name: ingress-nginx
        app.kubernetes.io/component: controller
    spec:
      serviceAccountName: ingress-nginx
      containers:
      - name: controller
        image: registry.k8s.io/ingress-nginx/controller:v1.9.4
        args:
        - /nginx-ingress-controller
        - --configmap=$(POD_NAMESPACE)/ingress-nginx-controller
        - --validating-webhook=:8443
        - --validating-webhook-certificate=/usr/local/certificates/cert
        - --validating-webhook-key=/usr/local/certificates/key
        ports:
        - name: http
          containerPort: 80
        - name: https
          containerPort: 443
        - name: webhook
          containerPort: 8443
        livenessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /healthz
            port: 10254
            scheme: HTTP
          initialDelaySeconds: 10
          periodSeconds: 10
        resources:
          limits:
            cpu: 1000m
            memory: 1Gi
          requests:
            cpu: 100m
            memory: 100Mi
        volumeMounts:
        - name: usrlocalcertificates
          mountPath: /usr/local/certificates
          readOnly: true
      volumes:
      - name: usrlocalcertificates
        emptyDir: {}

---
apiVersion: v1
kind: Service
metadata:
  name: ingress-nginx-controller
  namespace: ingress-nginx
spec:
  type: LoadBalancer
  externalTrafficPolicy: Cluster
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: http
  - name: https
    port: 443
    protocol: TCP
    targetPort: https
  selector:
    app.kubernetes.io/name: ingress-nginx
    app.kubernetes.io/component: controller
```

---

## ğŸ“¦ Helm Chart æ‰“åŒ…

### 1. Helm Chart ç»“æ„

```bash
# ç›®å½•ç»“æ„ç¤ºä¾‹
basebackend/
â”œâ”€â”€ Chart.yaml              # Chart å…ƒæ•°æ®
â”œâ”€â”€ values.yaml             # é»˜è®¤é…ç½®å€¼
â”œâ”€â”€ templates/              # æ¨¡æ¿ç›®å½•
â”‚   â”œâ”€â”€ _helpers.tpl        # åŠ©æ‰‹å‡½æ•°
â”‚   â”œâ”€â”€ deployment.yaml     # éƒ¨ç½²èµ„æº
â”‚   â”œâ”€â”€ service.yaml        # æœåŠ¡èµ„æº
â”‚   â”œâ”€â”€ ingress.yaml        # Ingress èµ„æº
â”‚   â”œâ”€â”€ configmap.yaml      # é…ç½®æ˜ å°„
â”‚   â”œâ”€â”€ secret.yaml         # å¯†é’¥
â”‚   â”œâ”€â”€ hpa.yaml            # æ°´å¹³è‡ªåŠ¨ä¼¸ç¼©
â”‚   â”œâ”€â”€ pdb.yaml            # PodDisruptionBudget
â”‚   â””â”€â”€ NOTES.txt           # è¯´æ˜æ–‡æ¡£
â”œâ”€â”€ charts/                 # ä¾èµ– Chart
â”œâ”€â”€ crds/                   # è‡ªå®šä¹‰èµ„æºå®šä¹‰
â””â”€â”€ templates.tests/        # æµ‹è¯•æ¨¡æ¿
```

### 2. Chart.yaml é…ç½®

```yaml
# Chart.yaml
apiVersion: v2
name: basebackend
description: BaseBackend å¾®æœåŠ¡å¹³å°
type: application
version: 1.0.0
appVersion: "1.0.0"
home: https://github.com/basebackend
sources:
  - https://github.com/basebackend/basebackend
maintainers:
  - name: basebackend-team
    email: team@basebackend.com
keywords:
  - microservices
  - spring-cloud
  - spring-boot
dependencies:
  - name: common
    version: "1.x.x"
    repository: https://charts.bitnami.com/bitnami
    alias: common
  - name: postgresql
    version: "12.x.x"
    repository: https://charts.bitnami.com/bitnami
    condition: postgresql.enabled
  - name: redis
    version: "18.x.x"
    repository: https://charts.bitnami.com/bitnami
    condition: redis.enabled
annotations:
  category: Application
  licenses: Apache-2.0
```

### 3. values.yaml é…ç½®

```yaml
# values.yaml
# å…¨å±€é…ç½®
global:
  imageRegistry: ""
  imagePullSecrets: []
  storageClass: ""

# é•œåƒé…ç½®
image:
  registry: docker.io
  repository: basebackend
  tag: "1.0.0"
  pullPolicy: IfNotPresent
  pullSecrets: []

# é•œåƒæ‹‰å–ç­–ç•¥
imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# æœåŠ¡é…ç½®
service:
  type: ClusterIP
  port: 80
  targetPort: 8080

# Ingress é…ç½®
ingress:
  enabled: true
  className: "nginx"
  annotations:
    cert-manager.io/cluster-issuer: letsencrypt-prod
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/force-ssl-redirect: "true"
  hosts:
    - host: api.basebackend.com
      paths:
        - path: /
          pathType: Prefix
  tls:
    - secretName: basebackend-tls
      hosts:
        - api.basebackend.com

# èµ„æºé™åˆ¶
resources:
  limits:
    cpu: 2000m
    memory: 2Gi
  requests:
    cpu: 1000m
    memory: 1Gi

# è‡ªåŠ¨ä¼¸ç¼©
autoscaling:
  enabled: true
  minReplicas: 2
  maxReplicas: 10
  targetCPUUtilizationPercentage: 70
  targetMemoryUtilizationPercentage: 80

# å¥åº·æ£€æŸ¥
livenessProbe:
  httpGet:
    path: /actuator/health
    port: 8080
  initialDelaySeconds: 120
  periodSeconds: 30
  timeoutSeconds: 10
  failureThreshold: 3

readinessProbe:
  httpGet:
    path: /actuator/health/readiness
    port: 8080
  initialDelaySeconds: 60
  periodSeconds: 10
  timeoutSeconds: 5
  failureThreshold: 3

# éƒ¨ç½²é…ç½®
replicaCount: 3

# èŠ‚ç‚¹é€‰æ‹©å™¨
nodeSelector: {}

# å®¹å¿åº¦
tolerations: []

# äº²å’Œæ€§
affinity:
  podAntiAffinity:
    preferredDuringSchedulingIgnoredDuringExecution:
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              app.kubernetes.io/name: basebackend
          topologyKey: kubernetes.io/hostname

# é…ç½®æ–‡ä»¶
config:
  # åº”ç”¨ç¨‹åºé…ç½®
  application:
    name: basebackend
    profiles:
      active: prod
  # æ•°æ®åº“é…ç½®
  database:
    host: ""
    port: 3306
    username: ""
    password: ""
    database: ""
  # Redis é…ç½®
  redis:
    host: ""
    port: 6379
    password: ""

# å¯†é’¥é…ç½®
secret:
  enabled: true
  annotations: {}
  labels: {}
  data: {}
  stringData: {}
  type: Opaque

# é…ç½®æ˜ å°„
configmap:
  enabled: true
  data: {}
  annotations: {}

# æœåŠ¡ç›‘æ§
monitoring:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 30s
    path: /actuator/prometheus
  podMonitor:
    enabled: false
    path: /metrics

# æƒé™é…ç½®
serviceAccount:
  create: true
  name: ""
  annotations: {}
  automountServiceAccountToken: true

# å®‰å…¨ä¸Šä¸‹æ–‡
securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop: ["ALL"]
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 1001
  fsGroup: 1001

# Pod å®‰å…¨ä¸Šä¸‹æ–‡
podSecurityContext:
  runAsNonRoot: true
  runAsUser: 1001
  fsGroup: 1001

# ç½‘ç»œç­–ç•¥
networkPolicy:
  enabled: true
  egress:
    enabled: false
    rules:
      - toPorts:
          - port: 80
          - port: 443
  ingress:
    enabled: true
    rules:
      - from:
          - namespaceSelector:
              matchLabels:
                name: ingress-nginx
        ports:
          - port: 8080

# PodDisruptionBudget
podDisruptionBudget:
  enabled: true
  minAvailable: 2

# æ°´å¹³ Pod è‡ªåŠ¨ç¼©æ”¾å™¨
hpa:
  enabled: true
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70

# Vertical Pod Autoscaler
vpa:
  enabled: false
  updateMode: "Auto"
  resources:
    limits:
      cpu: 2000m
      memory: 2Gi
    requests:
      cpu: 1000m
      memory: 1Gi

# Prometheus é…ç½®
prometheus:
  enabled: true
  serviceMonitor:
    enabled: true
    interval: 30s
    path: /actuator/prometheus
    additionalLabels: {}

# é™„åŠ å®¹å™¨
sidecarContainers: []

# é™„åŠ å·
extraVolumes: []

# é™„åŠ å·æŒ‚è½½
extraVolumeMounts: []

# åˆå§‹åŒ–å®¹å™¨
initContainers: []

# åç½®é’©å­
postStartHook:
  enabled: false
  exec:
    command:
      - /bin/sh
      - -c
      - echo "Application started"

# å‰ç½®é’©å­
preStopHook:
  enabled: false
  exec:
    command:
      - /bin/sh
      - -c
      - echo "Application stopping"

# æµ‹è¯•é…ç½®
tests:
  enabled: true
  image:
    repository: curlimages/curl
    tag: "8.4.0"
    pullPolicy: IfNotPresent
```

### 4. Deployment æ¨¡æ¿

```yaml
# templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "basebackend.fullname" . }}
  namespace: {{ .Release.Namespace | quote }}
  labels:
    {{- include "basebackend.labels" . | nindent 4 }}
  annotations:
    {{- with .Values.annotations }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  selector:
    matchLabels:
      {{- include "basebackend.selectorLabels" . | nindent 6 }}
  template:
    metadata:
      labels:
        {{- include "basebackend.selectorLabels" . | nindent 8 }}
      {{- if .Values.podLabels }}
        {{- toYaml .Values.podLabels | nindent 8 }}
      {{- end }}
      annotations:
        checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
        checksum/secret: {{ include (print $.Template.BasePath "/secret.yaml") . | sha256sum }}
        {{- with .Values.podAnnotations }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
    spec:
      {{- if .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml .Values.imagePullSecrets | nindent 8 }}
      {{- end }}
      serviceAccountName: {{ include "basebackend.serviceAccountName" . }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      {{- if .Values.initContainers }}
      initContainers:
        {{- tpl (toYaml .Values.initContainers) . | nindent 8 }}
      {{- end }}
      containers:
      - name: basebackend
        image: "{{ .Values.image.registry }}/{{ .Values.image.repository }}:{{ .Values.image.tag }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        ports:
        - name: http
          containerPort: 8080
          protocol: TCP
        - name: metrics
          containerPort: 9090
          protocol: TCP
        {{- if .Values.livenessProbe }}
        livenessProbe:
          {{- toYaml .Values.livenessProbe | nindent 10 }}
        {{- end }}
        {{- if .Values.readinessProbe }}
        readinessProbe:
          {{- toYaml .Values.readinessProbe | nindent 10 }}
        {{- end }}
        {{- if .Values.startupProbe }}
        startupProbe:
          {{- toYaml .Values.startupProbe | nindent 10 }}
        {{- end }}
        env:
        - name: SPRING_PROFILES_ACTIVE
          value: {{ .Values.config.application.profiles.active | quote }}
        - name: APPLICATION_NAME
          value: {{ .Values.config.application.name | quote }}
        {{- with .Values.extraEnvVars }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
        envFrom:
        {{- if .Values.configmap.enabled }}
        - configMapRef:
            name: {{ include "basebackend.fullname" . }}-config
        {{- end }}
        {{- if .Values.secret.enabled }}
        - secretRef:
            name: {{ include "basebackend.fullname" . }}-secret
        {{- end }}
        resources:
          {{- toYaml .Values.resources | nindent 10 }}
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: logs
          mountPath: /app/logs
        {{- if .Values.extraVolumeMounts }}
        {{- toYaml .Values.extraVolumeMounts | nindent 8 }}
        {{- end }}
        {{- if .Values.securityContext }}
        securityContext:
          {{- toYaml .Values.securityContext | nindent 10 }}
        {{- end }}
      {{- if .Values.sidecarContainers }}
      {{- tpl (toYaml .Values.sidecarContainers) . | nindent 6 }}
      {{- end }}
      volumes:
      - name: tmp
        emptyDir: {}
      - name: logs
        emptyDir: {}
      {{- if .Values.extraVolumes }}
      {{- toYaml .Values.extraVolumes | nindent 6 }}
      {{- end }}
      {{- if .Values.nodeSelector }}
      nodeSelector:
        {{- toYaml .Values.nodeSelector | nindent 8 }}
      {{- end }}
      {{- if .Values.tolerations }}
      tolerations:
        {{- toYaml .Values.tolerations | nindent 8 }}
      {{- end }}
      {{- if .Values.affinity }}
      affinity:
        {{- toYaml .Values.affinity | nindent 8 }}
      {{- end }}
      {{- if .Values.topologySpreadConstraints }}
      topologySpreadConstraints:
        {{- toYaml .Values.topologySpreadConstraints | nindent 8 }}
      {{- end }}
```

### 5. Service æ¨¡æ¿

```yaml
# templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: {{ include "basebackend.fullname" . }}
  namespace: {{ .Release.Namespace | quote }}
  labels:
    {{- include "basebackend.labels" . | nindent 4 }}
  {{- with .Values.service.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  type: {{ .Values.service.type }}
  ports:
  - port: {{ .Values.service.port }}
    targetPort: http
    protocol: TCP
    name: http
  {{- if .Values.metrics.enabled }}
  - port: {{ .Values.metrics.port }}
    targetPort: metrics
    protocol: TCP
    name: metrics
  {{- end }}
  selector:
    {{- include "basebackend.selectorLabels" . | nindent 4 }}
  {{- if eq .Values.service.type "LoadBalancer" }}
  loadBalancerSourceRanges:
    {{- toYaml .Values.service.loadBalancerSourceRanges | nindent 4 }}
  {{- end }}
```

### 6. åŠ©æ‰‹å‡½æ•°

```yaml
# templates/_helpers.tpl
{{/*
Expand the name of the chart.
*/}}
{{- define "basebackend.name" -}}
{{- default .Chart.Name .Values.nameOverride | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Create a default fully qualified app name.
*/}}
{{- define "basebackend.fullname" -}}
{{- if .Values.fullnameOverride }}
{{- .Values.fullnameOverride | trunc 63 | trimSuffix "-" }}
{{- else }}
{{- $name := default .Chart.Name .Values.nameOverride }}
{{- printf "%s-%s" .Release.Name $name | trunc 63 | trimSuffix "-" }}
{{- end }}
{{- end }}

{{/*
Create chart name and version as used by the chart label.
*/}}
{{- define "basebackend.chart" -}}
{{- printf "%s-%s" .Chart.Name .Chart.Version | replace "+" "_" | trunc 63 | trimSuffix "-" }}
{{- end }}

{{/*
Common labels
*/}}
{{- define "basebackend.labels" -}}
helm.sh/chart: {{ include "basebackend.chart" . }}
{{ include "basebackend.selectorLabels" . }}
{{- if .Chart.AppVersion }}
app.kubernetes.io/version: {{ .Chart.AppVersion | quote }}
{{- end }}
app.kubernetes.io/managed-by: {{ .Release.Service }}
{{- end }}

{{/*
Selector labels
*/}}
{{- define "basebackend.selectorLabels" -}}
app.kubernetes.io/name: {{ include "basebackend.name" . }}
app.kubernetes.io/instance: {{ .Release.Name }}
{{- end }}

{{/*
Create the name of the service account to use
*/}}
{{- define "basebackend.serviceAccountName" -}}
{{- if .Values.serviceAccount.create }}
{{- default (include "basebackend.fullname" .) .Values.serviceAccount.name }}
{{- else }}
{{- default "default" .Values.serviceAccount.name }}
{{- end }}
{{- end }}

{{/*
Return the proper Docker Image Registry Secret Names
*/}}
{{- define "basebackend.imagePullSecrets" -}}
{{- include "common.images.pullSecrets" (dict "images" .Values.image "global" .Values.global) }}
{{- end }}

{{/*
Create the name of the ConfigMap to use
*/}}
{{- define "basebackend.configmapName" -}}
{{- printf "%s-config" (include "basebackend.fullname" .) }}
{{- end }}

{{/*
Create the name of the Secret to use
*/}}
{{- define "basebackend.secretName" -}}
{{- printf "%s-secret" (include "basebackend.fullname" .) }}
{{- end }}

{{/*
Return true if we should create a ConfigMap
*/}}
{{- define "basebackend.createConfigMap" -}}
{{- if and .Values.configmap.enabled (or .Values.configmap.data .Values.config.existingConfigMap) }}
{{- true }}
{{- end }}
{{- end }}

{{/*
Return true if we should create a Secret
*/}}
{{- define "basebackend.createSecret" -}}
{{- if and .Values.secret.enabled (or .Values.secret.data .Values.secret.stringData .Values.config.existingSecret) }}
{{- true }}
{{- end }}
{{- end }}

{{/*
Validate values
*/}}
{{- define "basebackend.validateValues" -}}
{{- if not (hasKey .Values "replicaCount") }}
{{- fail "replicaCount is required" }}
{{- end }}
{{- end }}
```

### 7. å®Œæ•´æ€§æ£€æŸ¥

```bash
#!/bin/bash
# helm-validate.sh

log_info() {
    echo -e "\033[0;34m[INFO]\033[0m $1"
}

log_success() {
    echo -e "\033[0;32m[SUCCESS]\033[0m $1"
}

log_error() {
    echo -e "\033[0;31m[ERROR]\033[0m $1"
}

# éªŒè¯ Chart è¯­æ³•
validate_chart() {
    log_info "éªŒè¯ Chart è¯­æ³•..."

    if helm lint .; then
        log_success "Chart è¯­æ³•éªŒè¯é€šè¿‡"
    else
        log_error "Chart è¯­æ³•éªŒè¯å¤±è´¥"
        return 1
    fi
}

# æ¸²æŸ“æ¨¡æ¿
render_templates() {
    log_info "æ¸²æŸ“æ¨¡æ¿..."

    if helm template basebackend . --dry-run; then
        log_success "æ¨¡æ¿æ¸²æŸ“é€šè¿‡"
    else
        log_error "æ¨¡æ¿æ¸²æŸ“å¤±è´¥"
        return 1
    fi
}

# å®‰è£…æµ‹è¯•
test_install() {
    log_info "æµ‹è¯•å®‰è£…..."

    if helm install test-release . --dry-run; then
        log_success "å®‰è£…æµ‹è¯•é€šè¿‡"
    else
        log_error "å®‰è£…æµ‹è¯•å¤±è´¥"
        return 1
    fi
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log_info "æ£€æŸ¥ä¾èµ–..."

    if helm dependency list; then
        log_success "ä¾èµ–æ£€æŸ¥é€šè¿‡"
    else
        log_error "ä¾èµ–æ£€æŸ¥å¤±è´¥"
        return 1
    fi
}

# æ£€æŸ¥èµ„æº
check_resources() {
    log_info "æ£€æŸ¥èµ„æº..."

    resources=$(helm template basebackend . --show-only templates/deployment.yaml | grep "cpu\|memory" || true)

    if [ -n "$resources" ]; then
        log_success "èµ„æºæ£€æŸ¥é€šè¿‡"
        echo "$resources"
    else
        log_warn "æœªå‘ç°èµ„æºé™åˆ¶"
    fi
}

# ä¸»å‡½æ•°
main() {
    echo "========================================"
    echo "      Helm Chart éªŒè¯"
    echo "========================================"
    echo ""

    check_dependencies
    echo ""

    validate_chart
    echo ""

    render_templates
    echo ""

    test_install
    echo ""

    check_resources
    echo ""

    log_success "Chart éªŒè¯å®Œæˆï¼"
}

main "$@"
```

---

## ğŸ”„ CI/CD æµæ°´çº¿

### 1. GitLab CI/CD é…ç½®

```yaml
# .gitlab-ci.yml
variables:
  KUBECONFIG: /tmp/kubeconfig
  HELM_VERSION: "3.12.0"
  DOCKER_DRIVER: overlay2
  DOCKER_TMP_CERT_DIR: /certs/client

stages:
  - validate
  - build
  - test
  - package
  - deploy-dev
  - deploy-staging
  - deploy-prod
  - cleanup

# é»˜è®¤é…ç½®
.default_template: &default
  image: alpine/helm:${HELM_VERSION}
  before_script:
    - apk add --no-cache git curl openssh
    - mkdir -p ~/.ssh
    - echo "${SSH_PRIVATE_KEY}" | tr -d '\r' > ~/.ssh/id_rsa
    - chmod 600 ~/.ssh/id_rsa
    - ssh-keyscan ${K8S_CLUSTER_HOST} >> ~/.ssh/known_hosts
    - kubectl config set-cluster ${K8S_CLUSTER_NAME} --server=${K8S_CLUSTER_HOST}
    - kubectl config set-cluster ${K8S_CLUSTER_NAME} --insecure-skip-tls-verify=true
    - kubectl config set-credentials ${K8S_CLUSTER_USER} --token="${K8S_CLUSTER_TOKEN}"
    - kubectl config set-context ${K8S_CLUSTER_NAME} --cluster=${K8S_CLUSTER_NAME} --user=${K8S_CLUSTER_USER}
    - kubectl config use-context ${K8S_CLUSTER_NAME}
    - helm repo add bitnami https://charts.bitnami.com/bitnami
    - helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx
    - helm repo update

# ä»£ç éªŒè¯
validate:code:
  stage: validate
  image: alpine:latest
  script:
    - apk add --no-cache git
    - git fetch origin ${CI_COMMIT_BRANCH}
    - git diff --name-only origin/${CI_COMMIT_BRANCH}...${CI_COMMIT_SHA} | grep -E '\.(java|xml|yml|yaml)$' || true

validate:chart:
  stage: validate
  <<: *default
  script:
    - helm lint .

validate:yaml:
  stage: validate
  image: alpine/yq:latest
  script:
    - yq eval-all 'select(kind == "sequence" and all(.metadata.labels."app.kubernetes.io/name" == "basebackend"))' charts/*/values.yaml | \
      yq e 'all(. == true)' -

# Docker æ„å»º
build:docker:
  stage: build
  image: docker:latest
  services:
    - docker:dind
  before_script:
    - echo ${DOCKER_REGISTRY_PASSWORD} | docker login -u ${DOCKER_REGISTRY_USER} --password-stdin ${DOCKER_REGISTRY}
  script:
    - docker build -t ${DOCKER_REGISTRY}/${CI_PROJECT_PATH}:${CI_COMMIT_SHA} .
    - docker push ${DOCKER_REGISTRY}/${CI_PROJECT_PATH}:${CI_COMMIT_SHA}
    - docker tag ${DOCKER_REGISTRY}/${CI_PROJECT_PATH}:${CI_COMMIT_SHA} ${DOCKER_REGISTRY}/${CI_PROJECT_PATH}:${CI_COMMIT_BRANCH}
    - docker push ${DOCKER_REGISTRY}/${CI_PROJECT_PATH}:${CI_COMMIT_BRANCH}
  rules:
    - if: $CI_COMMIT_BRANCH
    - if: $CI_MERGE_REQUEST_IID

# å•å…ƒæµ‹è¯•
test:unit:
  stage: test
  image: maven:3.9-eclipse-temurin-17-jammy
  script:
    - mvn clean test -DskipTests=false
  artifacts:
    reports:
      junit:
        - target/surefire-reports/TEST-*.xml
  coverage: '/Code coverage: \d+\.\d+/'
  rules:
    - if: $CI_COMMIT_BRANCH

# é›†æˆæµ‹è¯•
test:integration:
  stage: test
  image: maven:3.9-eclipse-temurin-17-jammy
  script:
    - mvn clean verify -Pintegration-tests
  services:
    - name: mysql:8.0
      alias: mysql
    - name: redis:7
      alias: redis
  variables:
    MYSQL_ROOT_PASSWORD: "root"
    MYSQL_DATABASE: "basebackend_test"
  rules:
    - if: $CI_COMMIT_BRANCH

# å®‰å…¨æ‰«æ
security:scan:
  stage: test
  image: aquasec/trivy:latest
  script:
    - trivy image --exit-code 1 --severity HIGH,CRITICAL ${DOCKER_REGISTRY}/${CI_PROJECT_PATH}:${CI_COMMIT_SHA}
  rules:
    - if: $CI_COMMIT_BRANCH

# ä¾èµ–æ£€æŸ¥
dependency:scan:
  stage: test
  image: owasp/dependency-check:latest
  script:
    - dependency-check.sh --project "basebackend" --scan $(pwd) --enableRetired
  artifacts:
    paths:
      - reports/
  rules:
    - if: $CI_COMMIT_BRANCH

# Helm Package
package:helm:
  stage: package
  <<: *default
  script:
    - helm package .
    - helm repo index --url ${HELM_REPO_URL} .
  artifacts:
    paths:
      - "*.tgz"
      - index.yaml
  rules:
    - if: $CI_COMMIT_TAG
    - if: $CI_COMMIT_BRANCH == "main"

# éƒ¨ç½²åˆ°å¼€å‘ç¯å¢ƒ
deploy:dev:
  stage: deploy-dev
  <<: *default
  script:
    - helm upgrade --install basebackend-dev ./charts/basebackend \
      --namespace basebackend-dev \
      --create-namespace \
      --set image.tag=${CI_COMMIT_SHA} \
      --set ingress.hosts[0].host=api-dev.basebackend.com \
      --set resources.limits.cpu=500m \
      --set resources.limits.memory=512Mi \
      --wait --timeout=300s
    - kubectl rollout status deployment/basebackend-dev -n basebackend-dev --timeout=300s
  environment:
    name: development
    url: https://api-dev.basebackend.com
    deployment_tier: development
  rules:
    - if: $CI_COMMIT_BRANCH == "develop"

# éƒ¨ç½²åˆ°æµ‹è¯•ç¯å¢ƒ
deploy:staging:
  stage: deploy-staging
  <<: *default
  script:
    - helm upgrade --install basebackend-staging ./charts/basebackend \
      --namespace basebackend-staging \
      --create-namespace \
      --set image.tag=${CI_COMMIT_SHA} \
      --set ingress.hosts[0].host=api-staging.basebackend.com \
      --set replicaCount=3 \
      --set resources.limits.cpu=1000m \
      --set resources.limits.memory=1Gi \
      --set autoscaling.enabled=true \
      --set autoscaling.minReplicas=3 \
      --set autoscaling.maxReplicas=10 \
      --wait --timeout=600s
    - kubectl rollout status deployment/basebackend-staging -n basebackend-staging --timeout=600s
  environment:
    name: staging
    url: https://api-staging.basebackend.com
    deployment_tier: staging
  rules:
    - if: $CI_COMMIT_BRANCH == "develop"
  when: manual

# éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
deploy:prod:
  stage: deploy-prod
  <<: *default
  script:
    - helm upgrade --install basebackend-prod ./charts/basebackend \
      --namespace basebackend-prod \
      --create-namespace \
      --set image.tag=${CI_COMMIT_SHA} \
      --set ingress.hosts[0].host=api.basebackend.com \
      --set replicaCount=5 \
      --set resources.limits.cpu=2000m \
      --set resources.limits.memory=2Gi \
      --set autoscaling.enabled=true \
      --set autoscaling.minReplicas=5 \
      --set autoscaling.maxReplicas=20 \
      --set monitoring.enabled=true \
      --set networkPolicy.enabled=true \
      --set podDisruptionBudget.enabled=true \
      --wait --timeout=900s
    - kubectl rollout status deployment/basebackend-prod -n basebackend-prod --timeout=900s
  environment:
    name: production
    url: https://api.basebackend.com
    deployment_tier: production
  rules:
    - if: $CI_COMMIT_TAG
  when: manual

# å¥åº·æ£€æŸ¥
health:check:
  stage: deploy-prod
  image: alpine/curl:latest
  script:
    - curl -f https://api.basebackend.com/actuator/health
  environment:
    name: production
  rules:
    - if: $CI_COMMIT_TAG
  when: on_success

# å›æ»š
rollback:
  stage: cleanup
  <<: *default
  script:
    - helm rollback basebackend-prod -n basebackend-prod
    - kubectl rollout status deployment/basebackend-prod -n basebackend-prod
  environment:
    name: production
    url: https://api.basebackend.com
  rules:
    - if: $CI_PIPELINE_SOURCE == "web"
  when: manual

# æ¸…ç†æ—§çš„éƒ¨ç½²
cleanup:old:
  stage: cleanup
  image: alpine/helm:${HELM_VERSION}
  script:
    - helm list -n basebackend-dev --date --old | head -n -5 | awk '{print $1}' | \
      xargs -I {} helm uninstall {} -n basebackend-dev || true
  rules:
    - if: $CI_COMMIT_BRANCH == "develop"
  when: manual
```

### 2. GitHub Actions é…ç½®

```yaml
# .github/workflows/deploy.yml
name: Deploy to Kubernetes

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          java-version: '17'
          distribution: 'temurin'

      - name: Cache Maven dependencies
        uses: actions/cache@v4
        with:
          path: ~/.m2
          key: ${{ runner.os }}-m2-${{ hashFiles('**/pom.xml') }}
          restore-keys: ${{ runner.os }}-m2

      - name: Run tests
        run: mvn clean verify

      - name: Upload test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: test-results
          path: target/surefire-reports/

  build:
    needs: test
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Set up JDK 17
        uses: actions/setup-java@v4
        with:
          java-version: '17'
          distribution: 'temurin'

      - name: Log in to Container Registry
        uses: docker/login-action@v3
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-

      - name: Build and push Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          platforms: linux/amd64,linux/arm64

  validate-helm:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Set up Helm
        uses: azure/setup-helm@v4
        with:
          version: ${{ env.HELM_VERSION }}

      - name: Set up chart-testing
        uses: helm/chart-testing-action@v2.6.1

      - name: Lint chart
        run: ct lint

      - name: Render chart
        run: helm template basebackend ./charts/basebackend --dry-run

  deploy-dev:
    needs: [build, validate-helm]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: development

    steps:
      - uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Download Helm chart
        uses: actions/download-artifact@v4
        with:
          name: helm-chart
          path: charts/

      - name: Deploy to Development
        run: |
          helm upgrade --install basebackend-dev ./charts/basebackend \
            --namespace basebackend-dev \
            --create-namespace \
            --set image.tag=${{ github.sha }} \
            --set ingress.hosts[0].host=api-dev.basebackend.com \
            --wait --timeout=300s
        env:
          KUBECONFIG: ${{ secrets.KUBECONFIG_DEV }}

  deploy-staging:
    needs: [build, validate-helm]
    runs-on: ubuntu-latest
    if: github.ref == 'refs/heads/develop'
    environment: staging

    steps:
      - uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Deploy to Staging
        run: |
          helm upgrade --install basebackend-staging ./charts/basebackend \
            --namespace basebackend-staging \
            --create-namespace \
            --set image.tag=${{ github.sha }} \
            --set ingress.hosts[0].host=api-staging.basebackend.com \
            --wait --timeout=600s
        env:
          KUBECONFIG: ${{ secrets.KUBECONFIG_STAGING }}

  deploy-prod:
    needs: [build, validate-helm]
    runs-on: ubuntu-latest
    if: startsWith(github.ref, 'refs/tags/')
    environment: production

    steps:
      - uses: actions/checkout@v4

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4
        with:
          version: 'latest'

      - name: Deploy to Production
        run: |
          helm upgrade --install basebackend-prod ./charts/basebackend \
            --namespace basebackend-prod \
            --create-namespace \
            --set image.tag=${{ github.ref_name }} \
            --set ingress.hosts[0].host=api.basebackend.com \
            --wait --timeout=900s
        env:
          KUBECONFIG: ${{ secrets.KUBECONFIG_PROD }}

      - name: Health check
        run: |
          sleep 30
          curl -f https://api.basebackend.com/actuator/health
```

### 3. ArgoCD é…ç½®

```yaml
# argocd-application.yaml
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: basebackend
  namespace: argocd
  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:
  project: default
  source:
    repoURL: https://github.com/basebackend/basebackend.git
    targetRevision: HEAD
    path: charts/basebackend
    helm:
      valueFiles:
        - values-production.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: basebackend-prod
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - PruneLast=true
    retry:
      limit: 5
      backoff:
        duration: 5s
        factor: 2
        maxDuration: 3m

---
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: basebackend-staging
  namespace: argocd
spec:
  project: default
  source:
    repoURL: https://github.com/basebackend/basebackend.git
    targetRevision: develop
    path: charts/basebackend
    helm:
      valueFiles:
        - values-staging.yaml
  destination:
    server: https://kubernetes.default.svc
    namespace: basebackend-staging
  syncPolicy:
    automated:
      prune: true
      selfHeal: true
      allowEmpty: false
```

### 4. Jenkins Pipeline

```groovy
// Jenkinsfile
pipeline {
    agent any

    environment {
        REGISTRY = credentials('docker-registry')
        K8S_DEV = credentials('kubeconfig-dev')
        K8S_STAGING = credentials('kubeconfig-staging')
        K8S_PROD = credentials('kubeconfig-prod')
    }

    options {
        timeout(time: 60, unit: 'MINUTES')
        buildDiscarder(logRotator(numToKeepStr: '10'))
        timestamps()
    }

    stages {
        stage('Validate') {
            parallel {
                stage('Lint Helm') {
                    steps {
                        script {
                            sh 'helm lint charts/basebackend'
                        }
                    }
                }
                stage('Security Scan') {
                    steps {
                        script {
                            sh 'trivy fs --exit-code 1 --severity HIGH,CRITICAL .'
                        }
                    }
                }
            }
        }

        stage('Test') {
            parallel {
                stage('Unit Tests') {
                    steps {
                        script {
                            sh 'mvn clean test'
                        }
                    }
                    post {
                        always {
                            junit 'target/surefire-reports/*.xml'
                        }
                    }
                }
                stage('Integration Tests') {
                    steps {
                        script {
                            sh 'mvn clean verify -Pintegration-tests'
                        }
                    }
                }
            }
        }

        stage('Build & Push') {
            steps {
                script {
                    def image = docker.build("${REGISTRY_USR}/basebackend:${env.BUILD_NUMBER}")
                    docker.withRegistry("https://${REGISTRY_USR}", REGISTRY_PSW) {
                        image.push()
                        image.push('latest')
                    }
                }
            }
        }

        stage('Deploy Dev') {
            when {
                branch 'develop'
            }
            steps {
                script {
                    withKubeConfig([credentialsId: K8S_DEV, serverUrl: K8S_DEV_URL]) {
                        sh '''
                            helm upgrade --install basebackend-dev charts/basebackend \
                                --namespace basebackend-dev \
                                --create-namespace \
                                --set image.tag=${BUILD_NUMBER} \
                                --set ingress.hosts[0].host=api-dev.basebackend.com \
                                --wait --timeout=300s
                        '''
                    }
                }
            }
        }

        stage('Deploy Staging') {
            when {
                branch 'develop'
            }
            steps {
                script {
                    withKubeConfig([credentialsId: K8S_STAGING, serverUrl: K8S_STAGING_URL]) {
                        sh '''
                            helm upgrade --install basebackend-staging charts/basebackend \
                                --namespace basebackend-staging \
                                --create-namespace \
                                --set image.tag=${BUILD_NUMBER} \
                                --set ingress.hosts[0].host=api-staging.basebackend.com \
                                --wait --timeout=600s
                        '''
                    }
                }
            }
            post {
                always {
                    script {
                        sh '''
                            sleep 30
                            curl -f https://api-staging.basebackend.com/actuator/health
                        '''
                    }
                }
            }
        }

        stage('Deploy Prod') {
            when {
                buildingTag()
            }
            steps {
                script {
                    withKubeConfig([credentialsId: K8S_PROD, serverUrl: K8S_PROD_URL]) {
                        sh '''
                            helm upgrade --install basebackend-prod charts/basebackend \
                                --namespace basebackend-prod \
                                --create-namespace \
                                --set image.tag=${TAG_NAME} \
                                --set ingress.hosts[0].host=api.basebackend.com \
                                --wait --timeout=900s
                        '''
                    }
                }
            }
            post {
                always {
                    script {
                        sh '''
                            sleep 60
                            curl -f https://api.basebackend.com/actuator/health
                        '''
                    }
                }
            }
        }
    }

    post {
        always {
            cleanWs()
        }
        success {
            emailext (
                subject: "Deployment Successful: ${env.JOB_NAME} - ${env.BUILD_NUMBER}",
                body: "Deployment to ${env.GIT_BRANCH} completed successfully.",
                to: "${env.CHANGE_AUTHOR_EMAIL}"
            )
        }
        failure {
            emailext (
                subject: "Deployment Failed: ${env.JOB_NAME} - ${env.BUILD_NUMBER}",
                body: "Deployment to ${env.GIT_BRANCH} failed.",
                to: "${env.CHANGE_AUTHOR_EMAIL}"
            )
        }
    }
}
```

---

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### 1. é›†ç¾¤éªŒè¯è„šæœ¬

```bash
#!/bin/bash
# cluster-validation.sh

log_info() {
    echo -e "\033[0;34m[INFO]\033[0m $1"
}

log_success() {
    echo -e "\033[0;32m[SUCCESS]\033[0m $1"
}

log_error() {
    echo -e "\033[0;31m[ERROR]\033[0m $1"
}

# æ£€æŸ¥é›†ç¾¤èŠ‚ç‚¹
check_nodes() {
    log_info "æ£€æŸ¥é›†ç¾¤èŠ‚ç‚¹..."

    nodes=$(kubectl get nodes)
    echo "$nodes"

    if echo "$nodes" | grep -q "Ready"; then
        log_success "æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€æ­£å¸¸"
    else
        log_error "èŠ‚ç‚¹çŠ¶æ€å¼‚å¸¸"
        return 1
    fi
}

# æ£€æŸ¥ç³»ç»Ÿç»„ä»¶
check_system_components() {
    log_info "æ£€æŸ¥ç³»ç»Ÿç»„ä»¶..."

    components=$(kubectl get pods -n kube-system)
    echo "$components"

    if echo "$components" | grep -q "Running"; then
        log_success "ç³»ç»Ÿç»„ä»¶è¿è¡Œæ­£å¸¸"
    else
        log_error "ç³»ç»Ÿç»„ä»¶å¼‚å¸¸"
        return 1
    fi
}

# æ£€æŸ¥ Ingress
check_ingress() {
    log_info "æ£€æŸ¥ Ingress æ§åˆ¶å™¨..."

    ingress_pods=$(kubectl get pods -n ingress-nginx)
    echo "$ingress_pods"

    if echo "$ingress_pods" | grep -q "Running"; then
        log_success "Ingress æ§åˆ¶å™¨è¿è¡Œæ­£å¸¸"
    else
        log_error "Ingress æ§åˆ¶å™¨å¼‚å¸¸"
        return 1
    fi
}

# æ£€æŸ¥å­˜å‚¨
check_storage() {
    log_info "æ£€æŸ¥å­˜å‚¨ç±»..."

    storage_classes=$(kubectl get storageclass)
    echo "$storage_classes"

    if kubectl get storageclass | grep -q "default"; then
        log_success "é»˜è®¤å­˜å‚¨ç±»å·²é…ç½®"
    else
        log_warn "æœªæ‰¾åˆ°é»˜è®¤å­˜å‚¨ç±»"
    fi
}

# æ£€æŸ¥ç½‘ç»œæ’ä»¶
check_network_plugin() {
    log_info "æ£€æŸ¥ç½‘ç»œæ’ä»¶..."

    # æ£€æŸ¥ Flannel
    if kubectl get pods -n kube-flannel | grep -q "Running"; then
        log_success "Flannel ç½‘ç»œæ’ä»¶è¿è¡Œæ­£å¸¸"
        return 0
    fi

    # æ£€æŸ¥ Calico
    if kubectl get pods -n kube-system | grep -q "calico"; then
        log_success "Calico ç½‘ç»œæ’ä»¶è¿è¡Œæ­£å¸¸"
        return 0
    fi

    log_warn "æœªæ£€æµ‹åˆ°ç½‘ç»œæ’ä»¶"
}

# æ£€æŸ¥ç›‘æ§
check_monitoring() {
    log_info "æ£€æŸ¥ç›‘æ§ç»„ä»¶..."

    # æ£€æŸ¥ Prometheus
    if kubectl get pods -n monitoring | grep -q "prometheus"; then
        log_success "Prometheus è¿è¡Œæ­£å¸¸"
    else
        log_warn "Prometheus æœªéƒ¨ç½²"
    fi

    # æ£€æŸ¥ Grafana
    if kubectl get pods -n monitoring | grep -q "grafana"; then
        log_success "Grafana è¿è¡Œæ­£å¸¸"
    else
        log_warn "Grafana æœªéƒ¨ç½²"
    fi
}

# æ£€æŸ¥æœåŠ¡ç½‘æ ¼
check_service_mesh() {
    log_info "æ£€æŸ¥æœåŠ¡ç½‘æ ¼..."

    if kubectl get pods -n istio-system | grep -q "istiod"; then
        log_success "Istio æœåŠ¡ç½‘æ ¼è¿è¡Œæ­£å¸¸"
    else
        log_warn "Istio æœªéƒ¨ç½²"
    fi
}

# æ€§èƒ½æµ‹è¯•
performance_test() {
    log_info "æ‰§è¡Œæ€§èƒ½æµ‹è¯•..."

    # åˆ›å»ºæµ‹è¯•å‘½åç©ºé—´
    kubectl create namespace load-test --dry-run=client -o yaml | kubectl apply -f -

    # éƒ¨ç½²æµ‹è¯• Pod
    kubectl run nginx-test --image=nginx --namespace=load-test --restart=Never --dry-run=client -o yaml | kubectl apply -f -

    sleep 5

    # æ£€æŸ¥ Pod æ˜¯å¦è¿è¡Œ
    if kubectl get pods -n load-test | grep -q "Running"; then
        log_success "æ€§èƒ½æµ‹è¯•é€šè¿‡"
    else
        log_error "æ€§èƒ½æµ‹è¯•å¤±è´¥"
    fi

    # æ¸…ç†
    kubectl delete namespace load-test --ignore-not-found
}

# å‹åŠ›æµ‹è¯•
stress_test() {
    log_info "æ‰§è¡Œå‹åŠ›æµ‹è¯•..."

    # å®‰è£… stress å·¥å…·
    kubectl run stress-test --image=progrium/stress --namespace=default --restart=Never \
        -- --cpu 2 --io 1 --vm 2 --vm-bytes 128M --timeout 60s

    # ç­‰å¾…æµ‹è¯•å®Œæˆ
    kubectl logs stress-test --follow

    # æ¸…ç†
    kubectl delete pod stress-test --ignore-not-found
}

# ä¸»å‡½æ•°
main() {
    echo "========================================"
    echo "    Kubernetes é›†ç¾¤éªŒè¯"
    echo "========================================"
    echo ""

    check_nodes
    echo ""

    check_system_components
    echo ""

    check_ingress
    echo ""

    check_storage
    echo ""

    check_network_plugin
    echo ""

    check_monitoring
    echo ""

    check_service_mesh
    echo ""

    performance_test
    echo ""

    log_success "é›†ç¾¤éªŒè¯å®Œæˆï¼"
}

main "$@"
```

---

## ğŸ“š å‚è€ƒèµ„æ–™

1. [Kubernetes å®˜æ–¹æ–‡æ¡£](https://kubernetes.io/zh/docs/)
2. [Helm å®˜æ–¹æ–‡æ¡£](https://helm.sh/docs/)
3. [Kubernetes æœ€ä½³å®è·µ](https://kubernetes.io/zh/docs/concepts/configuration/manage-resources-containers/)
4. [ArgoCD å®˜æ–¹æ–‡æ¡£](https://argo-cd.readthedocs.io/)

---

## ğŸ“‹ å®¹å™¨åŒ–å®æ–½æ£€æŸ¥æ¸…å•

### Kubernetes é›†ç¾¤
- [ ] é›†ç¾¤èŠ‚ç‚¹å‡†å¤‡
- [ ] Kubernetes å®‰è£…
- [ ] ç½‘ç»œæ’ä»¶éƒ¨ç½²
- [ ] Ingress æ§åˆ¶å™¨é…ç½®
- [ ] å­˜å‚¨ç±»é…ç½®
- [ ] é›†ç¾¤éªŒè¯

### Helm Chart
- [ ] Chart ç»“æ„è®¾è®¡
- [ ] values.yaml é…ç½®
- [ ] æ¨¡æ¿ç¼–å†™
- [ ] åŠ©æ‰‹å‡½æ•°å®šä¹‰
- [ ] ä¾èµ–ç®¡ç†
- [ ] Chart éªŒè¯

### CI/CD æµæ°´çº¿
- [ ] ä»£ç è´¨é‡æ£€æŸ¥
- [ ] å•å…ƒæµ‹è¯•é›†æˆ
- [ ] Docker æ„å»º
- [ ] å®‰å…¨æ‰«æ
- [ ] Helm æ‰“åŒ…
- [ ] å¤šç¯å¢ƒéƒ¨ç½²
- [ ] å¥åº·æ£€æŸ¥
- [ ] å›æ»šæœºåˆ¶

### ç›‘æ§è¿ç»´
- [ ] Prometheus é…ç½®
- [ ] Grafana ä»ªè¡¨ç›˜
- [ ] æ—¥å¿—èšåˆ
- [ ] å‘Šè­¦è§„åˆ™
- [ ] æ€§èƒ½æµ‹è¯•
- [ ] å‹åŠ›æµ‹è¯•

---

**ç¼–åˆ¶ï¼š** æµ®æµ®é…± ğŸ±ï¼ˆçŒ«å¨˜å·¥ç¨‹å¸ˆï¼‰
**æ—¥æœŸï¼š** 2025-11-14
**çŠ¶æ€ï¼š** ğŸ“‹ æŒ‡å—å®Œæˆï¼Œå‡†å¤‡å®æ–½

**åŠ æ²¹å–µï½ å®¹å™¨åŒ–ä¸ç¼–æ’å³å°†å®Œæˆï¼** à¸…'Ï‰'à¸…
